{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU pypdf langchain-community langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RAG(Retrieval-Augmented Generation)에서, 전체 문서를 그대로 LLM에 넘기면 토큰 제한을 초과할 가능성이 큽니다.\n",
    "- 토큰 제한을 초과하면 모델이 입력을 잘라내거나 오류가 발생할 수 있습니다.\n",
    "- 비록 초과하지 않는다 해도, 불필요하게 긴 문서는 응답 생성 시간과 비용을 늘립니다.\n",
    "\n",
    "##### 문서를 적절한 단위(“chunk” 혹은 “paragraph”)로 나누는 이유\n",
    "- 한 번에 모델에 넣을 수 있는 텍스트 분량(맥시멈 토큰 수)을 넘지 않도록 나누어 처리합니다.\n",
    "- 검색(Retrieval) 시 사용자 질문과 가장 관련성 높은 문단만 골라서 LLM에 전달하므로, 토큰 사용량을 줄이고 답변 품질을 높입니다.\n",
    "\n",
    "##### Retriever가 Paragraph(또는 Chunk) 단위로 검색\n",
    "- 문서를 일정 크기로 “청크”로 나눈 뒤, 각 청크를 벡터 스토어 등에 저장합니다(임베딩).\n",
    "- 질문이 들어오면 관련성이 높은 청크만 골라 LLM에 전달합니다.\n",
    "- 이것이 “문단 단위로 필요한 만큼만” 모델에 주는 핵심 아이디어입니다.\n",
    "\n",
    "##### 정리:\n",
    "- 설명에 언급된 대로, 전체 문서를 통째로 보내면 토큰 초과나 비효율 문제가 생긴다.\n",
    "- 이를 피하려고 문서를 적절히 나눈 뒤, 질문과 관련된 부분만 LLM에 주는 방식이 RAG에서 일반적으로 쓰이는 방법이다.\n",
    "- Retriever(검색 단계)에서는 이렇게 나눈 “문단(청크)” 단위를 기반으로 검색·필터링하여 필요한 부분만 추출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_file_path = '../spring_framework_docs.pdf'\n",
    "loader = PyPDFLoader(pdf_file_path)\n",
    "pages = []\n",
    "# 쪽수별로 문서를 append\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 문서안에 테이블과 이미지와 같은 문서를 추출하기 위해 zerox 패키지를 사용\n",
    "# https://github.com/getomni-ai/zerox\n",
    "%pip install py-zerox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asyncio를 실행할 때, 이벤트루프가 없어야하나, notebook엣 default로 발생시키는 이벤트루프가 있어서, nest_asyncio를 사용하여 중첩된 이벤트루프를 사용할 수 있도록 함\n",
    "%pip install -q nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서를 AI가 파싱하여 텍스트로 변환하는데 사용하기 떄문에 가격이 매우매우 비싸다. 조심해서 사용해야함\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "# 가능하면 최상단에서 호출 (주피터/파이썬 REPL 등 환경에 따라서 순서가 중요할 수 있음)\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "try:\n",
    "    # UTF-8 출력이 가능한 환경(일반 파이썬 실행)에서만 적용\n",
    "    # 주피터/IPython 환경에서는 OutStream이어서 reconfigure 호출시 AttributeError가 날 수 있음\n",
    "    sys.stdout.reconfigure(encoding='utf-8')\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# .env 파일 로드 (OPENAI_API_KEY 등 환경 변수 확인)\n",
    "load_dotenv()\n",
    "\n",
    "# PyZeroX 임포트는 .env 로드 후에 하는 것을 권장\n",
    "from pyzerox import zerox\n",
    "\n",
    "\n",
    "def check_environment_variables() -> None:\n",
    "    \"\"\"\n",
    "    주 사용 환경 변수(예: OPENAI_API_KEY)가 제대로 설정되어 있는지 검사.\n",
    "    미설정 시 조기 종료 또는 사용자에게 경고 표시.\n",
    "    \"\"\"\n",
    "    openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not openai_key or not openai_key.startswith(\"sk-\"):\n",
    "        raise EnvironmentError(\n",
    "            \"OPENAI_API_KEY가 제대로 설정되어 있지 않거나 'sk-'로 시작하지 않습니다.\\n\"\n",
    "            \"비싼 API 호출 실패를 방지하기 위해 프로그램을 종료합니다.\"\n",
    "        )\n",
    "\n",
    "\n",
    "async def process_pdf(\n",
    "    file_path: str,\n",
    "    model: str = \"gpt-4o\",\n",
    "    output_dir: str = \"./documents\",\n",
    "    select_pages=None,\n",
    "    system_prompt=None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    PDF를 처리해서 결과를 반환하는 메인 비동기 함수.\n",
    "    \n",
    "    :param file_path: 분석할 PDF 파일 경로\n",
    "    :param model: 사용할 모델 (기본값 \"gpt-4o-mini\")\n",
    "    :param output_dir: 처리 결과(마크다운 등) 저장 경로\n",
    "    :param select_pages: 처리할 페이지 선택 (None이면 전체)\n",
    "    :param system_prompt: 시스템 프롬프트\n",
    "    :param kwargs: 모델별 추가 파라미터\n",
    "    :return: Zerox가 생성한 결과\n",
    "    \"\"\"\n",
    "    result = await zerox(\n",
    "        file_path=file_path,\n",
    "        model=model,\n",
    "        output_dir=output_dir,\n",
    "        custom_system_prompt=system_prompt,\n",
    "        select_pages=select_pages,\n",
    "        **kwargs\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    실제 실행을 담당하는 메인 함수.\n",
    "    \"\"\"\n",
    "    # 1) 환경 변수 점검\n",
    "    check_environment_variables()\n",
    "\n",
    "    # 2) 원하는 파라미터 설정\n",
    "    file_path = \"../spring_framework_docs.pdf\"\n",
    "    model_name = \"gpt-4o\"   # 필요 시 \"gpt-4\", \"gpt-3.5-turbo\" 등으로 교체\n",
    "    output_dir = \"./documents\"\n",
    "    select_pages = None  # None이면 모든 페이지 처리\n",
    "    system_prompt = None # 필요 시 사용자 정의 시스템 프롬프트 지정\n",
    "\n",
    "    # 3) 비동기 함수 실행\n",
    "    try:\n",
    "        result = asyncio.run(process_pdf(\n",
    "            file_path=file_path,\n",
    "            model=model_name,\n",
    "            output_dir=output_dir,\n",
    "            select_pages=select_pages,\n",
    "            system_prompt=system_prompt\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        print(f\"오류가 발생하여 작업을 중단합니다: {e}\")\n",
    "        return\n",
    "\n",
    "    # 4) 결과 출력 (너무 길 경우 파일 저장만 하고 간단히 요약만 보여줄 수도 있음)\n",
    "    print(result)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"unstructured[md]\" nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,  # 1500자씩 분할\n",
    "    chunk_overlap=100,  # 100자씩 중첩\n",
    "    separators=['\\n\\n', '\\n']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "markdown_path = \"./documents/spring_framework_docs.md\"\n",
    "loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "document_list = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q markdown html2text beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 마크다운 파일 경로\n",
    "markdown_path = './documents/spring_framework_docs.md'\n",
    "# 변환된 텍스트를 저장할 파일 경로\n",
    "text_path = './documents/spring_framework_docs.txt'\n",
    "\n",
    "# 마크다운 파일을 읽어옵니다\n",
    "with open(markdown_path, 'r', encoding='utf-8') as md_file:\n",
    "    md_content = md_file.read()\n",
    "\n",
    "# 마크다운 콘텐츠를 HTML로 변환합니다\n",
    "html_content = markdown.markdown(md_content)\n",
    "\n",
    "# HTML 콘텐츠를 파싱하여 텍스트만 추출합니다\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "text_content = soup.get_text()\n",
    "\n",
    "# 추출한 텍스트를 텍스트 파일로 저장합니다\n",
    "with open(text_path, 'w', encoding='utf-8') as txt_file:\n",
    "    txt_file.write(text_content)\n",
    "\n",
    "print(\"Markdown converted to plain text successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(text_path, encoding=\"utf-8\")\n",
    "document_list = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=document_list,\n",
    "    embedding=embeddings,\n",
    "    collection_name='spring_framework_docs',\n",
    "    persist_directory='./spring_framework_docs'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={'k': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"스프링 프레임워크란?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 가장 유사한 문서 3개를 검색\n",
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "graph_builder = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node는 2가지가 필요\n",
    "# 1. 문서를 가져오는 retrieve\n",
    "# 2. 답변을 생성하는 generate\n",
    "\n",
    "def retrieve(state: AgentState):\n",
    "    query = state['query']\n",
    "    docs = retriever.invoke(query)\n",
    "    return {'context': docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the LANGSMITH_API_KEY environment variable (create key in settings)\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model='gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: AgentState):\n",
    "    context = state['context']\n",
    "    query = state['query']\n",
    "    rag_chain = prompt | llm\n",
    "    response = rag_chain.invoke({'question': query, 'context': context})\n",
    "    response = llm.invoke(query, context)\n",
    "    return {'answer': response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node('retrieve', retrieve)\n",
    "graph_builder.add_node('generate', generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END\n",
    "\n",
    "graph_builder.add_edge(START, 'retrieve')\n",
    "graph_builder.add_edge('retrieve', 'generate')\n",
    "graph_builder.add_edge('generate', END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_graph_builder = StateGraph(AgentState).add_sequence([retrieve, generate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_graph_builder.add_edge(START, 'retrieve')\n",
    "sequence_graph_builder.add_edge('generate', END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_graph = sequence_graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {'query': query}\n",
    "sequence_graph.invoke(initial_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
